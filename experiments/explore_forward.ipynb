{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637dc5d0",
   "metadata": {},
   "source": [
    "## Notebook to explore forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7af02",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e14be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3cf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.transformer import Transformer\n",
    "from src.utils.mask import create_src_mask, create_tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd28f2",
   "metadata": {},
   "source": [
    "### Set the input & sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975eb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"apple tree\"\n",
    "seq_length = 20\n",
    "\n",
    "# seq length should be greater than input length + 2, otherwise cut off (comment assertion if wanted)\n",
    "assert seq_length >= len(input) + 2, \"Sequence length should be greater than input length + 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d51b2a",
   "metadata": {},
   "source": [
    "### Tokenize input and generate target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e4c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tokens: [1, 13, 28, 28, 24, 17, 97, 32, 30, 17, 17, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "Expected output: eert elppa\n",
      "Target tokens (full): [1, 17, 17, 30, 32, 97, 17, 24, 28, 28, 13, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenize the input\n",
    "src_tokens = tokenizer.tokenize(input, seq_length=seq_length)\n",
    "print(f\"Source tokens: {src_tokens}\")\n",
    "\n",
    "# Create expected output (here: reverse of input)\n",
    "expected_output = tokenizer.detokenize(src_tokens)[::-1]\n",
    "print(f\"Expected output: {expected_output}\")\n",
    "\n",
    "# Tokenize expected output\n",
    "tgt_tokens_full = tokenizer.tokenize(expected_output, seq_length=seq_length)\n",
    "print(f\"Target tokens (full): {tgt_tokens_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace2a17",
   "metadata": {},
   "source": [
    "### Create batch of src and tgt tokens, teacher-forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077233de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (20, 20)\n",
      "Source batch (first 3, as all entries are the same): \n",
      " [[ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      " [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      " [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]]\n",
      "---\n",
      "Target batch: \n",
      " [[ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  2]]\n"
     ]
    }
   ],
   "source": [
    "# create source and target batch\n",
    "src_batch = np.tile(src_tokens, (seq_length, 1))\n",
    "tgt_batch = []\n",
    "for i in range(1, seq_length + 1):\n",
    "    # reveal up to i tokens, pad the rest\n",
    "    tgt_row = tgt_tokens_full[:i] + [tokenizer.get_pad_token_id()] * (seq_length - i)\n",
    "    tgt_batch.append(tgt_row)\n",
    "tgt_batch = np.array(tgt_batch, dtype=np.int32)\n",
    "\n",
    "# (batch_size, seq_length)\n",
    "print(f\"Batch shape: {src_batch.shape}\")\n",
    "print(f\"Source batch (first 3, as all entries are the same): \\n {src_batch[:3]}\")\n",
    "print(\"---\")\n",
    "print(f\"Target batch: \\n {tgt_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b3bb8",
   "metadata": {},
   "source": [
    "### Setup parameters and build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4b6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer parameters\n",
    "src_vocab_size = tokenizer.vocab_size()\n",
    "tgt_vocab_size = tokenizer.vocab_size()\n",
    "src_seq_len = seq_length\n",
    "tgt_seq_len = seq_length\n",
    "d_model = 64\n",
    "n_blocks = 6\n",
    "n_heads = 8\n",
    "dropout_rate = 0.1\n",
    "d_ff = 2048\n",
    "#adjust seed to get different results\n",
    "seed = 456\n",
    "\n",
    "# Build the Transformer model\n",
    "transformer = Transformer.build_transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_seq_len,\n",
    "    tgt_seq_len,\n",
    "    d_model,\n",
    "    n_blocks,\n",
    "    n_heads,\n",
    "    dropout_rate,\n",
    "    d_ff,\n",
    "    seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff92ba7",
   "metadata": {},
   "source": [
    "### Create masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882e5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Source mask (first 3 rows, as all rows are the same): \n",
      " [[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1]]\n",
      "---\n",
      "Example Target mask: \n",
      " [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "src_masks = np.stack(\n",
    "    [\n",
    "        create_src_mask(src_batch[i], tokenizer.get_pad_token_id(), n_heads)\n",
    "        for i in range(seq_length)\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "tgt_masks = np.stack(\n",
    "    [\n",
    "        create_tgt_mask(tgt_batch[i], tokenizer.get_pad_token_id(), n_heads)\n",
    "        for i in range(seq_length)\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "print(f\"Example Source mask (first 3 rows, as all rows are the same): \\n {src_masks[-1][-1][:3]}\")\n",
    "print(\"---\")\n",
    "print(f\"Example Target mask: \\n {tgt_masks[-1][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f7e4d",
   "metadata": {},
   "source": [
    "### Forward pass with created batches and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d4ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (20, 20, 103)\n"
     ]
    }
   ],
   "source": [
    "# Set transformer to evaluation mode (as we pretend to be in inference mode)\n",
    "transformer.eval()\n",
    "\n",
    "output = transformer(src_batch, tgt_batch, src_masks, tgt_masks)\n",
    "print(f\"Output shape: {output.shape}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f17fe",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "Since the transformer model is untrained, its predictions are essentially random and do not correspond to the expected output. However, because we set the model to evaluation mode (disabling dropout and other stochastic layers), the outputs are deterministic and may show repeated patterns across the batch.\n",
    "\n",
    "Below are some sample outputs for each batch element. You can change the random seed in the model initialization to observe different untrained outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5f10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  Predicted token IDs: [28 94 28 28 28 28 28 28 94 94 94 94 94 94 94 94 94 94 94 94]\n",
      "  Detokenized:         p|pppppp||||||||||||\n",
      "---\n",
      "1th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94]\n",
      "  Detokenized:         po||||||||||||||||||\n",
      "---\n",
      "2th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 21 21 21 21 89 89 94 94 21 21 21 94 94 94 94 94 94]\n",
      "  Detokenized:         pooiiii]]||iii||||||\n",
      "---\n",
      "3th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94]\n",
      "  Detokenized:         poo|||||||||||||||||\n",
      "---\n",
      "4th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94 94]\n",
      "  Detokenized:         poo|||||||||||||||||\n",
      "---\n",
      "5th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 94 21 21 21 21 21 21 21 21 21 21 21 21 21]\n",
      "  Detokenized:         poo||i|iiiiiiiiiiiii\n",
      "---\n",
      "6th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 21 21 21 21 21 21 21 21 21 21 21 21 21]\n",
      "  Detokenized:         poo||ioiiiiiiiiiiiii\n",
      "---\n",
      "7th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 21 21 21 21 21 21 21 21 94 94 94 21]\n",
      "  Detokenized:         poo||io|iiiiiiii|||i\n",
      "---\n",
      "8th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 21 21 21 21 21 21 21 94 94 94 21]\n",
      "  Detokenized:         poo||io||iiiiiii|||i\n",
      "---\n",
      "9th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28  0  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "10th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "11th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "12th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "13th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "14th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "15th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "16th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "17th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "18th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  0]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n",
      "19th element of batch:\n",
      "  Source input: [ 1 13 28 28 24 17 97 32 30 17 17  0  0  0  0  0  0  0  0  2]\n",
      "  Target input: [ 1 17 17 30 32 97 17 24 28 28 13  0  0  0  0  0  0  0  0  2]\n",
      "  Predicted token IDs: [28 27 27 94 94 21 27 94 94 94 21 21 21 21 21 94 94 94 94 21]\n",
      "  Detokenized:         poo||io|||iiiii||||i\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "pred_token_ids = np.argmax(output, axis=-1)\n",
    "\n",
    "# Iterate over batch \n",
    "for i in range(pred_token_ids.shape[0]):\n",
    "    pred_ids = pred_token_ids[i]\n",
    "    pred_text = tokenizer.detokenize(pred_ids.tolist())\n",
    "    print(f\"{i}th element of batch:\")\n",
    "    print(\"  Source input:\", src_batch[i])\n",
    "    print(\"  Target input:\", tgt_batch[i])\n",
    "    print(\"  Predicted token IDs:\", pred_ids)\n",
    "    print(\"  Detokenized:        \", pred_text)\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
